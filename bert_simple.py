# -*- coding: utf-8 -*-
"""BERt Simple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QM-E7Z2Ou1aJEiF5hcwp6ChmrE-_bTg5
"""



!git clone https://github.com/nishi267/data.git

!pip install simpletransformers

import pandas as pd

data=pd.read_csv('/content/data/ner_dataset.csv', encoding='latin1')

data.head()

data=data.fillna(method="ffill")

data.head(30)

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data['Sentence #']=LabelEncoder().fit_transform(data["Sentence #"])

data.head(30)

data.rename(columns={"Sentence #":"sentence_id","Word":"words","Tag":"labels"}, inplace=True)

data["labels"]=data["labels"].str.upper()

X=data[["sentence_id","words"]]
Y=data["labels"]

x_train, x_test, y_train, y_test=train_test_split(X,Y, test_size=0.2)

train_data=pd.DataFrame({"sentence_id":x_train["sentence_id"],"words":x_train["words"], "labels":y_train})
test_data=pd.DataFrame({"sentence_id":x_test["sentence_id"],"words":x_test["words"], "labels":y_test})

train_data.head()

from simpletransformers.ner import NERModel, NERArgs

label=data["labels"].unique().tolist()

label

args=NERArgs()
args.num_train_epochs=1
args.learning_rate=1e-4
args.overwrite_output_dir=True
args.train_batch_size=32
args.eval_batch_size=32



model=NERModel('bert', 'bert-base-cased', labels=label, args=args)

model.train_model(train_data, eval_data=test_data, acc=accuracy_score)

result, model_outputs, preds_list=model.eval_model(test_data)

result

prediction, model_output=model.predict(["This is Nishi"])

prediction

!pip install bert-extractive-summarizer

!pip install wikipedia

import wikipedia

wiki = wikipedia.page('Amsterdam')
article=wiki.content
print(article)

from summarizer import Summarizer
model = Summarizer()
result = model(article, min_length=30,max_length=300)
summary = "".join(result)
print(summary)

!pip install newspaper3k

from newspaper import fulltext
import requests
article_url="https://stackoverflow.com/questions/50560395/how-to-install-cuda-in-google-colab-gpus"
article = fulltext(requests.get(article_url).text)
print(article)

from summarizer import Summarizer
model = Summarizer()
result = model(article, min_length=30,max_length=300)
summary = "".join(result)
print(summary)

article_url="https://en.wikipedia.org/wiki/Yogi_Adityanath"
article = fulltext(requests.get(article_url).text)
print(article)

# Import package
import re
# Clean text
text = re.sub(r'==.*?==+', '', article)
text = text.replace('\n', '')
text

from summarizer import Summarizer
model = Summarizer()
result = model(text, min_length=300,max_length=500)
summary = "".join(result)
print(summary)

wiki = wikipedia.page('Yogi_Adityanath')
article=wiki.content
print(article)

from summarizer import Summarizer
model = Summarizer()
result = model(article, min_length=300,max_length=500)
summary = "".join(result)
print(summary)

